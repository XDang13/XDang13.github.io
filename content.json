{"meta":{"title":"XDang","subtitle":"","description":"","author":"西葫芦炖冬瓜","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"强化学习（一）","slug":"强化学习（一）","date":"2019-12-17T14:32:49.000Z","updated":"2019-12-17T14:40:58.525Z","comments":true,"path":"2019/12/17/强化学习（一）/","link":"","permalink":"http://yoursite.com/2019/12/17/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"前言首先，我先默认这个系列的读者都已经具备了基础的机器学习和深度学习知识（线性代数，概率论，机器学习，神经网络等等），所以在涉及到这些基础知识和专业术语的时候就不会详细解释了。 其次，这一系列主要是参考D.Silver在UCL的Reinforcement Learning课程和R.Sutton的《An Introduction to Reinforcement Learning》教材进行。整个系列主要是跟着D.Silver的课程大纲走。所以，希望更加系统和全面学习强化学习的朋友可以去B站上观看D.Silver的课程，同时可以购买R.Sutton的教材（目前已经有了中译版）作为补充。 最后，如果大家发现了任何问题或者有哪里看不懂的，可以给我留言，我会随缘回复的。 什么是强化学习基本概念首先，我们先来了解下什么是强化学习。强化学习，也可以叫做增强学习，作为几年来热度非常高的名词，相信研究人工智能的朋友或多或少的都听过。其中最为出名应该是AlphaGo以及进化版的AlphaGo Zero了。接下来我们来介绍一下强化学习是什么，以及它和监督学习与非监督学习有什么不同。 简单来说强化学习就是指学习如何在一个任务或者环境中达到最大化的收益。学习者不会被提前告知什么是好的行为，什么是坏的行为，必须通过自己的不断探索发现哪些行为会带来正面的收益，哪些行为会带来负面的收益，并且最终通过采取一些好的行为来获取最大的收益。比起其他的机器学习方式，强化学习更加接近于自然界中动物的学习方式，不断与周围环境互动，学习如何能更好的生存下去。 在进行下一面的内容前，我们先来明确一下强化学习有哪几重含义。首先，强化学习可以是指一个问题，比如我们可以说围棋AI是一个强化学习问题。其次，强化学习也是一类解决上述问题的方法，比如我们可以说AlphaGo运用了强化学习来解决围棋AI问题。最后，强化学习也可以是指研究此类问题和解决方法的研究领域。 对比我们常用的监督学习学习，强化学习通常在学习过程没有已经标注好的数据集进行辅助，只能在不断地与环境进行交互中学习。通常监督学习所学习到的知识只能泛化到类似的场景中，比如图像识别中，如果在实际场景中，光线的明亮程度变化太大，则会降低判断的准确度，但是我们可以轻易的收集到大量在不同光线下具有代表性的数据。而在强化学习问题中，我们很难去收集到大量即正确又具有代表性的数据，比如我们在玩儿星际争霸时，我们可能会遇到成千上万中不同的情形，而且我们也很难去判断这些情形中哪些是类似的，哪些是有代表性的。所以，在强化学习中，要达到收益最大化的目标，只能通过自身与环境互动来学习。同样，强化学习也不同于无监督学习。无监督学习要解决的问题是在未标注的数据中找出隐含结构，比如KMeans聚类算法。而强化学习则是要解决如何获取最大收益的问题。因此，我们可以认为强化学习是机器学习中的第三类范式。 例子强化学习最具影响力的项目要数Alpha Go了，但Alpha Go和Alpha Go Master都并非单独使用了强化学习进行训练，在训练过程还用到了监督学习进行预训练。在之后的版本Alpha Go Zero和Alpha Zero则是完全使用强化学习进行训练。之后会单独写一篇文章来简单讲解一下Alpha Go系列。DeepMind的另一个强化学习代表项目则是2019年年初的Alpha Star星际争霸2项目。与之相对的则是OpenAI的Open AI Five Dota2项目。 强化学习问题的组成部分一个强化学习问题主要是有智能体（Agent，也成了代理）和环境构成。而整个问题要解决的问题也就是智能体如何在环境中采取一系列的行动获取最大的奖励。 Learning 在整个强化学习问题中，智能体通过观察环境中状态来作出相应的动作来与环境互动，做出动作之后，智能体则可以获得相应的奖励以及改变后的环境状态，以此循环往复，直至达成目标或者达成终止条件。在整个过程中，要明确一下几点： 奖励 状态 环境 奖励我们首先来介绍一下奖励。奖励可以说是整个强化学习问题中最为重要的部分。在整个强化学习中，奖励是唯一可以表明智能体表现好坏的信号，而智能体最终的目标就是获取最多的奖励。可以说整个强化学习都是建立在奖励之上。通常我们将奖励用$ R_{t} $表示。同时我们还要注意一点就是在强化学习中，奖励可能会有一定的延迟性，当前的一个行为可能要在之后几轮交互后才能体现出好坏。在强化学习中，我们需要注意有些时候可能牺牲当前的奖励来换取未来长期的奖励是一个更好的选择，例如投入时间来读博可能会少挣一些钱，但是长期来看读博会拓展未来的职业道路。 状态在我们介绍状态的概念前，先来介绍一下历史。历史就是指一系列观察信息，动作和奖励。可以用以下的公式来表示：$$H_t= O_1,R_1,A_1,…A_{t-1},O_t,R_t$$而接下来发生什么则依赖于历史，智能体选择的动作，对应的奖励和下一步观察到的信息。 而所谓的状态就是指用来决定接下来发生什么的信息，也可以用以下公式来表达：$$S_t=f(H_t)$$ 强化学习的四大要素策略在强化学习中，策略就是 收益信号价值函数模型（可选）强化学习代理的分类按有无策略和价值函数区分按有无模型区分","categories":[],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}]},{"title":"test","slug":"test","date":"2019-12-16T13:46:59.000Z","updated":"2019-12-16T13:46:59.072Z","comments":true,"path":"2019/12/16/test/","link":"","permalink":"http://yoursite.com/2019/12/16/test/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-12-16T13:42:00.286Z","updated":"2019-12-16T13:48:01.234Z","comments":true,"path":"2019/12/16/hello-world/","link":"","permalink":"http://yoursite.com/2019/12/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment$$M = a_{1} / b^{2}$$","categories":[],"tags":[]}]}