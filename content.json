{"meta":{"title":"XDang","subtitle":"","description":"","author":"西葫芦炖冬瓜","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"年度总结","slug":"年度总结","date":"2020-01-02T14:03:43.000Z","updated":"2020-01-02T14:04:10.855Z","comments":true,"path":"2020/01/02/年度总结/","link":"","permalink":"http://yoursite.com/2020/01/02/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","excerpt":"","text":"2019 2020年已至，新的一年新的开始，又要设立一批新的Flag了。在设立新的Flag前，回顾下我的2019年，总结下去年的Flag。 2019年，简单的一句话概括就是我体验了一把瞎猫撞上死耗子。很不幸，我这回扮演的角色是死耗子。 18年年底说要多读书，19上半年年也算是按照计划在进行了，读了《侏罗纪公园》，《远大前程》，《时光边缘的男人》等小说，还复习部分微积分，线代。年中则由于各种问题，不得不暂停读书计划，虽然之后还是读了一些闲书。好在年底断断续续的又开始了读书计划。希望20年可以长期保持读书这个习惯，毕竟书中自有黄金屋，书中自有颜如玉。 同样，我的工程技术和学术水平也类似于我的读书计划，高开低走。19年开始的几个月里还是延续2018年的势头，做了五子棋版的AlphaGo，看了一些3D点云目标检测的算法，还开始把一些常用的深度学习算法写成个人常用库。之后则是由于同样的原因，不得不中断这些项目。同时中断的还有论文的阅读。下半年也是断断续续又开始了。 2019年对一句话是深有感触：人无信不立，业无信不兴。 2019年中，也认识到了身体才是革命的本钱。下半年身体健康方面出了些不大不小的问题，花了很长时间去找病因，各种检查也是做了不少。好在不是什么严重问题。今后是要开始注意身体健康了。 基本上能想到就这些了。 怎么说，倒不是我想写的这么无聊，这么丧，主要是2019年的确也没个啥值得太高兴的事，也没啥可以写的很欢乐的事。 希望2020能够把新的Flag一一实现吧。","categories":[],"tags":[]},{"title":"强化学习（一）","slug":"强化学习（一）","date":"2019-12-17T14:32:49.000Z","updated":"2019-12-18T14:01:38.963Z","comments":true,"path":"2019/12/17/强化学习（一）/","link":"","permalink":"http://yoursite.com/2019/12/17/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"前言首先，我先默认这个系列的读者都已经具备了基础的机器学习和深度学习知识（线性代数，概率论，机器学习，神经网络等等），所以在涉及到这些基础知识和专业术语的时候就不会详细解释了。 其次，这一系列主要是参考D.Silver在UCL的Reinforcement Learning课程和R.Sutton的《An Introduction to Reinforcement Learning》教材进行。整个系列主要是跟着D.Silver的课程大纲走。所以，希望更加系统和全面学习强化学习的朋友可以去B站上观看D.Silver的课程，同时可以购买R.Sutton的教材（目前已经有了中译版）作为补充。 最后，如果大家发现了任何问题或者有哪里看不懂的，可以给我留言，我会随缘回复的。 什么是强化学习基本概念首先，我们先来了解下什么是强化学习。强化学习，也可以叫做增强学习，作为几年来热度非常高的名词，相信研究人工智能的朋友或多或少的都听过。其中最为出名应该是AlphaGo以及进化版的AlphaGo Zero了。接下来我们来介绍一下强化学习是什么，以及它和监督学习与非监督学习有什么不同。 简单来说强化学习就是指学习如何在一个任务或者环境中达到最大化的收益。学习者不会被提前告知什么是好的行为，什么是坏的行为，必须通过自己的不断探索发现哪些行为会带来正面的收益，哪些行为会带来负面的收益，并且最终通过采取一些好的行为来获取最大的收益。比起其他的机器学习方式，强化学习更加接近于自然界中动物的学习方式，不断与周围环境互动，学习如何能更好的生存下去。 在进行下一面的内容前，我们先来明确一下强化学习有哪几重含义。首先，强化学习可以是指一个问题，比如我们可以说围棋AI是一个强化学习问题。其次，强化学习也是一类解决上述问题的方法，比如我们可以说AlphaGo运用了强化学习来解决围棋AI问题。最后，强化学习也可以是指研究此类问题和解决方法的研究领域。 对比我们常用的监督学习学习，强化学习通常在学习过程没有已经标注好的数据集进行辅助，只能在不断地与环境进行交互中学习。通常监督学习所学习到的知识只能泛化到类似的场景中，比如图像识别中，如果在实际场景中，光线的明亮程度变化太大，则会降低判断的准确度，但是我们可以轻易的收集到大量在不同光线下具有代表性的数据。而在强化学习问题中，我们很难去收集到大量即正确又具有代表性的数据，比如我们在玩儿星际争霸时，我们可能会遇到成千上万中不同的情形，而且我们也很难去判断这些情形中哪些是类似的，哪些是有代表性的。所以，在强化学习中，要达到收益最大化的目标，只能通过自身与环境互动来学习。同样，强化学习也不同于无监督学习。无监督学习要解决的问题是在未标注的数据中找出隐含结构，比如KMeans聚类算法。而强化学习则是要解决如何获取最大收益的问题。因此，我们可以认为强化学习是机器学习中的第三类范式。 例子强化学习最具影响力的项目要数Alpha Go了，但Alpha Go和Alpha Go Master都并非单独使用了强化学习进行训练，在训练过程还用到了监督学习进行预训练。在之后的版本Alpha Go Zero和Alpha Zero则是完全使用强化学习进行训练。之后会单独写一篇文章来简单讲解一下Alpha Go系列。DeepMind的另一个强化学习代表项目则是2019年年初的Alpha Star星际争霸2项目。与之相对的则是OpenAI的Open AI Five Dota2项目。 强化学习问题的组成部分一个强化学习问题主要是有智能体（Agent，也成了代理）和环境构成。而整个问题要解决的问题也就是智能体如何在环境中采取一系列的行动获取最大的奖励。 Reinforcement Learning Problem.png 在整个强化学习问题中，智能体通过观察环境中状态来作出相应的动作来与环境互动，做出动作之后，智能体则可以获得相应的奖励以及改变后的环境状态，以此循环往复，直至达成目标或者达成终止条件。在整个过程中，要明确一下几点： 奖励 状态 奖励我们首先来介绍一下奖励。奖励可以说是整个强化学习问题中最为重要的部分。在整个强化学习中，奖励是唯一可以表明智能体表现好坏的信号，而智能体最终的目标就是获取最多的奖励。可以说整个强化学习都是建立在奖励之上。通常我们将奖励用$ R_{t} $表示。同时我们还要注意一点就是在强化学习中，奖励可能会有一定的延迟性，当前的一个行为可能要在之后几轮交互后才能体现出好坏。在强化学习中，我们需要注意有些时候可能牺牲当前的奖励来换取未来长期的奖励是一个更好的选择，例如投入时间来读博可能会少挣一些钱，但是长期来看读博会拓展未来的职业道路。 状态在我们介绍状态的概念前，先来介绍一下历史。历史就是指一系列观察信息，动作和奖励。可以用以下的公式来表示：$$H_t= O_1,R_1,A_1,…A_{t-1},O_t,R_t$$而接下来发生什么则依赖于历史，智能体选择的动作，对应的奖励和下一步观察到的信息。 而所谓的状态就是指用来决定接下来发生什么的信息，也可以用以下公式来表达：$$S_t=f(H_t)$$ 要注意的一点是环境的状态和智能体的能观察到的状态是不一样的。环境的状态$S_{t}^{e}$指的是环境私有的表示，一般情况下智能体是不能直接观察到环境内部的状态，即使环境内部的状态是可见的，通常也会包含许多无用的信息。而智能体可观察到的状态$S_{t}^{a}$则是我们在强化学习中用到的状态。当智能体可以完全观察到环境状态的时候，我们称该环境为完全可见环境（Fully Observable Environment）。反之，我们称该环境为不完全可见环境（Partially Observable Environment）。举例来说在一局星际争霸2比赛中，环境状态$S_{t}^{e}$除了包含我们可见的画面外，还包括了网络延迟，双方的ID，IP地址等信息，而智能体可观察到的状态$S_{t}^{a}$则是指游戏画面或者由游戏开放的API所提供的信息。 在强化学习问题中，我们一般默认状态是马尔可夫状态，即当前状态包含历史是所有有用的信息。其数学定义为：$$当且仅当状态S_{t}满足P[S_{t+1}|S_{t}] = P[S_{t+1}|S_{1},…,S_{t}]时，状态S_{t}为马尔可夫状态。$$当一个状态是马尔可夫状态时，未来可能的状态与当前状态是独立的，而且当前状态包含了过去所有状态的有用信息。 强化学习的四大要素策略在强化学习中，策略就是指状态到动作的映射。策略可以是一张查询表，一个简单的函数，也可以包含一段复杂的计算过程，例如深度神经网络。强化学习的策略可以是决定性的策略$a = \\pi(s)$，也可以是随机性的策略$\\pi(a|s) = P[A_{t}=a|S_{t}=a]$。 收益信号收益信号就是我们前文中提到的奖励。在强化学习中，收益信号定义了对于智能体，什么是好，什么是坏。而智能体的学习目标则是最大化收益信号。所以收益信号是改变策略的基础。 价值函数类似于收益信号，价值函数也是强化学习用于表示收益的工具。不同于收益信号的是，价值函数是从长远角度来看什么是好，什么是坏的。一个状态的价值是指智能体从当前状态开始，在未来可以累积的总收益的期望，即$$v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + … | S_{t}=s ]$$要收益高的当前状态并不意味着有着高的价值。比如说，在星际争霸2中，我们消灭对方的一个单位的收益是10，而采矿的收益是1，从长远来看，在开局时，用农民去采矿的价值要高于用农民去rush的价值。 模型（可选）强化学习中的最后一个要素是模型。在这里，模型指的是强化学习中对环境的建模。当给定一个状态和动作是，模型可以预测下一个状态和奖励。模型通常用于规划 - 考虑未来可能的情形来预先选择动作。在最近的研究中，模型也可以用于进行模拟交互来收集数据，即智能体通过不仅与环境交互来学习，也要通过与模型交互来学习。注意，在强化学习系统中，模型不是必须的。 强化学习代理的分类由于强化学习智能体中，至少要包含策略，价值函数中一个。同时，也可以包含模型或不包含模型。按照其组成成分可以按两种模式来分类 - 有无策略和价值函数区分和有无模型区分。 按有无策略和价值函数区分按有无策略和价值函数区分，智能体可以分为： 基于价值的强化学习（Value-Based）： 无策略 有价值函数 基于策略的强化学习（Policy-Based）： 有策略 无价值函数 演员评论家强化学习（Actor-Critic）： 有策略 有价值函数 按有无模型区分按有无模型区分，则可以分为： 无模型强化学习（Model Free）： 至少包含策略或价值函数中的一个 无模型 基于模型的强化学习（Model Based）： 至少包含策略或价值函数中的一个 有模型","categories":[],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"}]}]}